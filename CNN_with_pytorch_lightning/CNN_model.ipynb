{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CUDA!\n",
      "\n",
      "Epoch [1/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:15<00:00, 16.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.9547] [Acc = 78.26%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.4629] [Acc = 90.21%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[297  24   1   8  26  15   8   2  56  43]\n",
      " [ 13 437   2   1   4   7   0   2   4   3]\n",
      " [  0   4 440   5   0   3   2   0   5  20]\n",
      " [  1   4   0 455   8   4   0   1   2   4]\n",
      " [ 31   7   0   9 350  13   1   0  57  10]\n",
      " [  9  21   3   7  14 321  12   5   3  81]\n",
      " [  3   5   0   3   6   8 417   2   2  32]\n",
      " [  5   6   1   2   0  10   3 396   7  46]\n",
      " [ 31  14   0   4  30   3   1   1 391   3]\n",
      " [ 29  14   8   9   9  50  17   9   5 329]]\n",
      "\n",
      "precision : 0.8081\n",
      "recall : 0.8027\n",
      "f1 : 0.8033\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [2/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:15<00:00, 16.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.4358] [Acc = 89.27%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.3411] [Acc = 91.97%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[379   4   2   2  21   9   3   2  25  33]\n",
      " [  3 452   2   1   1   4   2   0   4   4]\n",
      " [  1   0 475   0   0   0   0   0   0   3]\n",
      " [  2   1   1 467   4   0   1   0   3   0]\n",
      " [ 25   1   0  11 400   6   0   0  31   4]\n",
      " [ 13   8   0   3   6 385   9   2   2  48]\n",
      " [  3   0   0   2   0   7 463   0   0   3]\n",
      " [  1   0   3   0   0   7   4 444   0  17]\n",
      " [ 13   2   0   5  23   0   1   0 434   0]\n",
      " [ 16   5   4   2   5  38  13   6   4 386]]\n",
      "\n",
      "precision : 0.8973\n",
      "recall : 0.8973\n",
      "f1 : 0.8969\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [3/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:15<00:00, 15.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.3338] [Acc = 90.83%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.2660] [Acc = 92.60%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[404   0   1   4  13   7   2   1  21  27]\n",
      " [ 10 446   2   0   0  10   2   0   0   3]\n",
      " [  0   0 477   1   0   0   0   1   0   0]\n",
      " [  0   0   0 473   1   1   2   0   1   1]\n",
      " [ 27   0   0   7 408   8   1   0  24   3]\n",
      " [ 11   5   0   0   6 408   4   3   0  39]\n",
      " [  0   0   0   1   2   8 462   0   0   5]\n",
      " [  1   0   0   0   0   7   2 446   0  20]\n",
      " [ 19   0   0   4  15   1   0   0 439   0]\n",
      " [ 17   4   3   2   3  39  14   7   1 389]]\n",
      "\n",
      "precision : 0.9121\n",
      "recall : 0.9113\n",
      "f1 : 0.9115\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [4/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 15.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.2962] [Acc = 91.61%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.2519] [Acc = 93.35%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[404   3   0   1  19  11   2   0  19  21]\n",
      " [  7 459   1   0   0   2   1   0   1   2]\n",
      " [  1   0 470   2   0   3   0   1   0   2]\n",
      " [  3   0   0 469   5   1   0   0   0   1]\n",
      " [ 18   2   0   5 415   7   1   0  23   7]\n",
      " [  3   4   1   3   4 413   8   3   2  35]\n",
      " [  0   0   1   1   0   2 466   3   1   4]\n",
      " [  0   0   2   0   0   5   2 451   0  16]\n",
      " [ 14   1   0   4  16   0   1   0 442   0]\n",
      " [ 15   5   4   0   3  30  10   8   4 400]]\n",
      "\n",
      "precision : 0.9190\n",
      "recall : 0.9190\n",
      "f1 : 0.9189\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [5/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.2632] [Acc = 92.46%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 17.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.2331] [Acc = 93.10%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[408   2   1   4  22   3   3   1  16  20]\n",
      " [  2 458   1   0   1   5   3   0   2   1]\n",
      " [  0   0 474   2   0   1   0   0   0   2]\n",
      " [  2   0   0 474   1   0   0   0   0   2]\n",
      " [ 22   2   0   5 415   2   2   0  25   5]\n",
      " [  2   7   0   1   5 418   8   2   0  33]\n",
      " [  3   0   0   1   1   6 461   1   0   5]\n",
      " [  2   0   1   0   0   7   3 453   0  10]\n",
      " [ 10   0   0   4  12   1   0   0 450   1]\n",
      " [ 11   2   4   0   1  30  13   5   3 410]]\n",
      "\n",
      "precision : 0.9257\n",
      "recall : 0.9257\n",
      "f1 : 0.9256\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [6/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.2361] [Acc = 93.04%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.2251] [Acc = 93.73%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[417   2   1   2  17   1   2   0  16  22]\n",
      " [  8 457   0   0   0   4   0   0   2   2]\n",
      " [  1   0 475   0   0   0   1   0   0   2]\n",
      " [  0   0   0 474   3   2   0   0   0   0]\n",
      " [ 18   3   0   6 420   6   1   0  16   8]\n",
      " [  7   6   0   1   3 417   6   2   1  33]\n",
      " [  0   0   0   1   2   3 467   0   0   5]\n",
      " [  3   0   2   0   0   4   2 454   0  11]\n",
      " [  9   1   0   1  10   1   0   0 456   0]\n",
      " [ 16   1   3   0   3  26  10   6   2 412]]\n",
      "\n",
      "precision : 0.9318\n",
      "recall : 0.9316\n",
      "f1 : 0.9316\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [7/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.2396] [Acc = 92.76%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1995] [Acc = 94.23%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[416   0   0   2  12   6   2   2  19  21]\n",
      " [  3 459   0   1   2   4   0   0   0   4]\n",
      " [  0   0 474   0   0   1   1   0   0   3]\n",
      " [  0   0   1 472   1   1   2   0   0   2]\n",
      " [ 20   3   0   2 427   3   2   0  18   3]\n",
      " [  3   3   0   1   2 419   4   2   1  41]\n",
      " [  2   0   0   0   1   3 466   1   0   5]\n",
      " [  3   0   1   0   0   3   1 451   1  16]\n",
      " [ 12   0   0   1  14   0   1   0 448   2]\n",
      " [ 13   4   2   0   5  29   9   5   2 410]]\n",
      "\n",
      "precision : 0.9309\n",
      "recall : 0.9301\n",
      "f1 : 0.9303\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [8/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:17<00:00, 14.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.2310] [Acc = 92.39%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 18.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.2107] [Acc = 93.60%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[410   3   0   4  13   9   2   0  18  21]\n",
      " [  4 457   1   0   2   5   0   0   1   3]\n",
      " [  0   0 474   1   0   2   0   0   1   1]\n",
      " [  1   1   0 467   5   0   1   0   0   4]\n",
      " [ 19   1   0   3 428   7   2   0  15   3]\n",
      " [  5   6   0   1   3 416   1   5   1  38]\n",
      " [  1   0   0   0   3   4 465   0   0   5]\n",
      " [  0   0   0   0   0   4   2 454   0  16]\n",
      " [ 12   0   0   1  15   1   0   0 448   1]\n",
      " [ 20   4   1   1   4  32   8   4   2 403]]\n",
      "\n",
      "precision : 0.9264\n",
      "recall : 0.9259\n",
      "f1 : 0.9261\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [9/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.2146] [Acc = 93.42%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1965] [Acc = 94.48%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[411   4   1   3  14   6   2   3  17  19]\n",
      " [  6 459   0   0   0   5   0   0   1   2]\n",
      " [  0   0 476   0   0   1   0   0   0   2]\n",
      " [  0   0   0 476   0   1   1   0   1   0]\n",
      " [ 17   1   0   1 426   6   2   0  17   8]\n",
      " [  7   0   1   1   0 432   7   2   0  26]\n",
      " [  1   0   0   0   0   3 467   1   0   6]\n",
      " [  3   0   2   1   1   2   2 452   0  13]\n",
      " [  9   2   0   2  12   2   1   0 449   1]\n",
      " [ 10   2   2   2   3  22  10   5   1 422]]\n",
      "\n",
      "precision : 0.9363\n",
      "recall : 0.9360\n",
      "f1 : 0.9360\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [10/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.2100] [Acc = 93.26%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1863] [Acc = 94.60%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[426   3   0   3  15   3   1   1  14  14]\n",
      " [  3 458   0   0   1   5   0   0   1   5]\n",
      " [  1   1 475   0   0   1   0   0   0   1]\n",
      " [  0   0   0 473   2   2   0   0   0   2]\n",
      " [ 21   2   1   4 420   3   3   0  21   3]\n",
      " [  2   2   1   1   0 431   1   2   0  36]\n",
      " [  1   0   0   0   3   6 464   0   0   4]\n",
      " [  2   0   1   0   0   2   2 455   0  14]\n",
      " [  5   0   0   1  15   1   1   0 452   3]\n",
      " [ 18   3   1   2   3  22   8  10   1 411]]\n",
      "\n",
      "precision : 0.9352\n",
      "recall : 0.9349\n",
      "f1 : 0.9350\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [11/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 15.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.1828] [Acc = 94.57%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1956] [Acc = 94.73%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[420   3   1   2  12   6   1   0  18  17]\n",
      " [  2 462   0   0   2   3   1   0   0   3]\n",
      " [  0   0 477   1   0   0   0   1   0   0]\n",
      " [  0   0   0 476   1   0   1   0   0   1]\n",
      " [ 15   0   0   2 434   4   0   0  19   4]\n",
      " [  3   3   1   1   1 445   2   1   1  18]\n",
      " [  0   0   0   0   0   3 468   1   0   6]\n",
      " [  0   0   1   0   0   5   1 459   0  10]\n",
      " [ 15   0   0   1  10   1   0   0 451   0]\n",
      " [  2   2   6   1   3  24   8   5   2 426]]\n",
      "\n",
      "precision : 0.9461\n",
      "recall : 0.9460\n",
      "f1 : 0.9459\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [12/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.1929] [Acc = 93.89%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1936] [Acc = 94.35%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[420   0   1   1  10  10   2   2  14  20]\n",
      " [  3 464   0   0   0   3   0   0   2   1]\n",
      " [  0   0 478   0   0   0   0   0   0   1]\n",
      " [  0   0   0 473   1   2   1   0   2   0]\n",
      " [ 16   0   1   3 429   4   2   0  19   4]\n",
      " [  5   2   1   2   2 422   6   3   1  32]\n",
      " [  1   0   0   0   0   5 470   0   1   1]\n",
      " [  0   0   1   1   0   3   1 456   0  14]\n",
      " [ 11   0   0   1  10   1   0   0 452   3]\n",
      " [ 13   2   3   2   3  18   8   4   2 424]]\n",
      "\n",
      "precision : 0.9401\n",
      "recall : 0.9397\n",
      "f1 : 0.9398\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [13/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.1813] [Acc = 93.89%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1888] [Acc = 94.35%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[413   2   0   2  20   7   2   0  17  17]\n",
      " [  3 462   0   0   1   5   0   0   1   1]\n",
      " [  1   0 476   0   0   1   0   0   0   1]\n",
      " [  0   0   0 477   1   0   0   0   1   0]\n",
      " [ 25   2   0   1 430   3   0   0  14   3]\n",
      " [  3   5   2   1   2 433   5   0   1  24]\n",
      " [  0   0   0   0   3   3 468   0   0   4]\n",
      " [  1   0   0   0   0   5   3 458   0   9]\n",
      " [  9   0   0   0  12   2   0   0 454   1]\n",
      " [ 15   2   1   0   2  25  12   3   2 417]]\n",
      "\n",
      "precision : 0.9398\n",
      "recall : 0.9398\n",
      "f1 : 0.9397\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [14/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:17<00:00, 14.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.1817] [Acc = 94.70%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1926] [Acc = 94.48%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[427   2   0   1  11   8   2   1  12  16]\n",
      " [  3 462   0   0   0   4   0   0   0   4]\n",
      " [  0   0 477   0   0   1   0   0   0   1]\n",
      " [  0   0   0 477   1   0   0   0   1   0]\n",
      " [ 19   1   0   1 433   4   1   0  16   3]\n",
      " [  3   3   0   1   1 439   4   4   2  19]\n",
      " [  1   0   0   0   5   4 465   0   0   3]\n",
      " [  0   0   1   0   0   4   3 454   0  14]\n",
      " [  9   0   0   2   8   1   0   0 456   2]\n",
      " [ 10   2   2   1   3  17   5   7   1 431]]\n",
      "\n",
      "precision : 0.9469\n",
      "recall : 0.9466\n",
      "f1 : 0.9467\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [15/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:17<00:00, 14.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.1873] [Acc = 93.97%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1980] [Acc = 93.98%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[420   1   0   2  11   8   2   1  18  17]\n",
      " [  3 461   0   0   0   6   0   0   1   2]\n",
      " [  0   0 475   0   0   2   0   0   0   2]\n",
      " [  0   1   0 471   4   0   0   0   3   0]\n",
      " [ 23   1   0   2 438   3   0   0   9   2]\n",
      " [  3   3   0   3   6 424   3   1   1  32]\n",
      " [  1   0   0   0   3   4 467   1   0   2]\n",
      " [  2   0   1   0   0   5   1 458   0   9]\n",
      " [  6   1   0   0  10   1   1   0 458   1]\n",
      " [ 13   2   3   0   4  22  10   8   1 416]]\n",
      "\n",
      "precision : 0.9398\n",
      "recall : 0.9397\n",
      "f1 : 0.9397\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [16/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 15.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.1803] [Acc = 94.57%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1955] [Acc = 94.35%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[424   2   0   2   8   9   5   0  14  16]\n",
      " [  5 459   0   0   0   3   0   0   0   6]\n",
      " [  0   0 476   0   0   2   0   0   0   1]\n",
      " [  0   0   0 472   3   1   0   0   0   3]\n",
      " [ 15   3   0   5 436   2   3   0  11   3]\n",
      " [  5   3   0   0   0 442   4   1   0  21]\n",
      " [  2   0   0   0   1   2 469   0   0   4]\n",
      " [  2   1   0   0   0   6   1 455   0  11]\n",
      " [ 10   0   0   1   9   0   0   1 456   1]\n",
      " [  8   1   1   0   2  23  12   5   1 426]]\n",
      "\n",
      "precision : 0.9458\n",
      "recall : 0.9454\n",
      "f1 : 0.9455\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [17/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.1727] [Acc = 94.65%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1880] [Acc = 94.35%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[431   0   1   1   8   5   2   0  15  17]\n",
      " [  3 459   0   0   3   4   0   0   1   3]\n",
      " [  0   0 477   0   0   1   0   0   0   1]\n",
      " [  0   0   0 474   2   1   0   0   1   1]\n",
      " [ 15   0   0   2 428   4   1   0  19   9]\n",
      " [  4   1   0   0   2 444   3   1   0  21]\n",
      " [  0   0   0   0   1   2 473   0   0   2]\n",
      " [  0   0   0   0   0   5   2 459   0  10]\n",
      " [ 13   3   0   2   9   0   1   0 449   1]\n",
      " [  8   3   1   0   6  20   8   8   1 424]]\n",
      "\n",
      "precision : 0.9463\n",
      "recall : 0.9460\n",
      "f1 : 0.9461\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [18/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.1801] [Acc = 94.40%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1907] [Acc = 94.60%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[420   2   0   2  15   8   1   0  15  17]\n",
      " [  5 460   0   0   1   4   0   0   0   3]\n",
      " [  0   0 476   0   0   1   0   0   0   2]\n",
      " [  1   0   0 475   1   0   0   0   1   1]\n",
      " [ 15   1   0   1 438   1   0   0  17   5]\n",
      " [  0   3   0   0   4 429   5   7   0  28]\n",
      " [  1   2   0   1   0   7 464   0   0   3]\n",
      " [  3   0   1   0   0   5   2 456   0   9]\n",
      " [  6   0   0   0   9   0   0   0 463   0]\n",
      " [ 11   3   2   0   3  15   9   6   1 429]]\n",
      "\n",
      "precision : 0.9446\n",
      "recall : 0.9443\n",
      "f1 : 0.9443\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [19/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.1787] [Acc = 94.30%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.1937] [Acc = 94.35%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[422   1   1   2  15   9   0   2  12  16]\n",
      " [  2 461   0   0   2   3   0   0   1   4]\n",
      " [  0   0 477   0   0   1   0   0   0   1]\n",
      " [  0   0   1 475   1   0   1   0   1   0]\n",
      " [ 22   0   0   4 431   4   1   0  14   2]\n",
      " [  5   2   0   1   0 435   4   1   3  25]\n",
      " [  0   0   0   0   1   6 466   0   0   5]\n",
      " [  1   0   1   0   0   2   1 460   0  11]\n",
      " [  9   0   0   2  10   1   0   0 455   1]\n",
      " [ 11   3   1   0   2  23   8   8   1 422]]\n",
      "\n",
      "precision : 0.9432\n",
      "recall : 0.9431\n",
      "f1 : 0.9431\n",
      "__________________________________________________ \n",
      "\n",
      "Epoch [20/20]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 249/249 [00:16<00:00, 14.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [Loss = 0.1809] [Acc = 94.32%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:02<00:00, 19.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: [Loss = 0.2036] [Acc = 93.98%]\n",
      "\n",
      "Confusion matrix :\n",
      "[[429   0   0   1   7   8   2   0  16  17]\n",
      " [  4 459   1   0   0   3   1   0   1   4]\n",
      " [  0   0 479   0   0   0   0   0   0   0]\n",
      " [  0   0   0 475   2   1   0   0   0   1]\n",
      " [ 17   2   0   3 425   8   2   0  18   3]\n",
      " [  4   2   3   1   2 433   2   0   1  28]\n",
      " [  0   0   0   0   4   4 462   0   0   8]\n",
      " [  0   0   1   0   0   7   2 458   0   8]\n",
      " [ 10   1   0   0  11   1   0   1 453   1]\n",
      " [ 10   2   0   0   3  21   9   5   0 429]]\n",
      "\n",
      "precision : 0.9432\n",
      "recall : 0.9427\n",
      "f1 : 0.9428\n",
      "__________________________________________________ \n",
      "\n",
      "Training complete in 6m 26s\n",
      "Best validation Acc: 94.73%\n",
      "Correct : {'SeaRays': 27, 'JellyFish': 320, 'SeaUrchins': 90, 'Otter': 20, 'Penguin': 2, 'Seahorse': 0, 'Crabs': 18, 'StarFish': 19, 'Dolphin': 221, 'Octopus': 49}\n",
      "Wrong : {'SeaRays': 10, 'JellyFish': 45, 'SeaUrchins': 9, 'Otter': 0, 'Penguin': 0, 'Seahorse': 0, 'Crabs': 1, 'StarFish': 0, 'Dolphin': 75, 'Octopus': 33}\n",
      "Total : {'SeaRays': 37, 'JellyFish': 365, 'SeaUrchins': 99, 'Otter': 20, 'Penguin': 2, 'Seahorse': 0, 'Crabs': 19, 'StarFish': 19, 'Dolphin': 296, 'Octopus': 82}\n",
      "Test acc : 81.58%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tempfile import TemporaryDirectory\n",
    "from pandas.core.common import flatten\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from sklearn.metrics import precision_score, f1_score, recall_score, confusion_matrix\n",
    "\n",
    "cudnn.benchmark = True\n",
    "plt.ion()\n",
    "writer = SummaryWriter('runs')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(\"Running on CUDA!\\n\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Running on CPU!\\n\")\n",
    "\n",
    "train_data_path = \"./Dataset/Train/\" \n",
    "test_data_path = \"./Dataset/Validation/\"\n",
    "\n",
    "class_names = []\n",
    "test_image_paths = []\n",
    "train_image_paths = []\n",
    "\n",
    "for data_path in glob.glob(train_data_path + '/*'):\n",
    "    class_names.append(data_path.split('/')[-1]) \n",
    "    train_image_paths.append(glob.glob(data_path + '/*.jpg'))\n",
    "    \n",
    "for data_path in glob.glob(test_data_path + '/*'):\n",
    "    test_image_paths.append(glob.glob(data_path + '/*.jpg'))\n",
    "\n",
    "train_image_paths = list(flatten(train_image_paths))\n",
    "test_image_paths = list(flatten(test_image_paths))\n",
    "random.shuffle(train_image_paths)\n",
    "\n",
    "idx_to_class = {i:j for i, j in enumerate(class_names)}\n",
    "class_to_idx = {value:key for key,value in idx_to_class.items()}\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.image_paths[idx]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        label = image_filepath.split('/')[-2]\n",
    "        label = class_to_idx[label]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "data_transforms = {\n",
    "    \"Train\": A.Compose([\n",
    "        A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=0.5),\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.MultiplicativeNoise(multiplier=[0.5,2], per_channel=True, p=0.2),\n",
    "        A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "        A.HorizontalFlip(),\n",
    "        A.VerticalFlip(),\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    \"Validation\": A.Compose([\n",
    "        A.Resize(224, 224),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "image_datasets = {\n",
    "    \"Train\" : Dataset(train_image_paths, data_transforms[\"Train\"]),\n",
    "    \"Validation\" : Dataset(test_image_paths, data_transforms[\"Validation\"]),\n",
    "}\n",
    "dataloaders = {x:DataLoader(image_datasets[x], batch_size=16, shuffle=True) for x in [\"Train\", \"Validation\"]}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"Train\", \"Validation\"]}\n",
    "inputs, classes = next(iter(dataloaders[\"Validation\"]))\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(inputs)\n",
    "writer.add_image('sea_animals_images', img_grid)\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}]\\n')\n",
    "\n",
    "            mean_acc = 0.0\n",
    "            mean_loss = 0.0\n",
    "            class_labels = []\n",
    "            class_preds = []\n",
    "            for phase in [\"Train\", \"Validation\"]:\n",
    "                if phase == \"Train\":\n",
    "                    model.train()\n",
    "                else:\n",
    "                    model.eval()\n",
    "                    \n",
    "                \n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                for i, (inputs, labels) in enumerate(tqdm(dataloaders[phase])):\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == \"Train\"):\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                        if phase == \"Train\":\n",
    "                            optimizer.zero_grad()\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                        class_probs_batch = [F.softmax(output, dim=0) for output in outputs]\n",
    "\n",
    "                        class_preds.append(class_probs_batch)\n",
    "                        class_labels.append(labels)\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "\n",
    "                if phase == \"Train\":\n",
    "                    scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase] * 100\n",
    "\n",
    "                mean_loss += epoch_loss\n",
    "                mean_acc += epoch_acc\n",
    "\n",
    "                print(f'{phase}: [Loss = {epoch_loss:.4f}] [Acc = {epoch_acc:.2f}%]\\n')\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == \"Validation\" and epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            class_preds = torch.cat([torch.stack(batch) for batch in class_preds])\n",
    "            class_labels = torch.cat(class_labels)\n",
    "\n",
    "            np_labels = class_labels.cpu().detach().numpy()\n",
    "            np_preds = class_preds.cpu().detach().numpy()\n",
    "            np_preds = np.argmax(np_preds, axis=1)\n",
    "\n",
    "            cm = confusion_matrix(np_labels, np_preds)\n",
    "            precision = precision_score(np_labels, np_preds, average=\"macro\")\n",
    "            recall = recall_score(np_labels, np_preds, average=\"macro\")\n",
    "            f1 = f1_score(np_labels, np_preds, average=\"macro\")\n",
    "\n",
    "            writer.add_scalar('avg_accuracy', mean_acc / 2, epoch)\n",
    "            writer.add_scalar('avg_loss', mean_loss / 2, epoch)\n",
    "            writer.add_scalar('avg_precision', precision, epoch)\n",
    "            writer.add_scalar('avg_recall', recall, epoch)\n",
    "            writer.add_scalar('avg_f1', f1, epoch)\n",
    "\n",
    "            print(f\"Confusion matrix :\\n{cm}\\n\")\n",
    "            print(f\"precision : {precision:.4f}\")\n",
    "            print(f\"recall : {recall:.4f}\")\n",
    "            print(f\"f1 : {f1:.4f}\")\n",
    "            \n",
    "            classes = range(10)\n",
    "            for i in classes:\n",
    "                labels_i = class_labels == i\n",
    "                preds_i = class_preds[:, i]\n",
    "                writer.add_pr_curve(str(i), labels_i, preds_i, global_step=1)\n",
    "                writer.close()\n",
    "\n",
    "            print('_' * 50,\"\\n\")\n",
    "            \n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best validation Acc: {best_acc:.2f}%')\n",
    "\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "        \n",
    "    return model\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "def visualize_model(model, num_images=10):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloaders[\"Validation\"]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for i in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                imshow(inputs.cpu().data[i], f'predicted: {class_names[preds[i]]}')\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "                \n",
    "        model.train(mode=was_training)\n",
    "\n",
    "def visualize_model_predictions(model,img_path):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = data_transforms[\"Validation\"](image=img)[\"image\"]\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        imshow(img.cpu().data[0], f'Predicted: {class_names[preds[0]]}')\n",
    "\n",
    "        model.train(mode=was_training)\n",
    "\n",
    "model_conv = models.resnet50(weights='IMAGENET1K_V2')\n",
    "\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "model_conv = model_conv.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_conv = optim.Adam(model_conv.fc.parameters(), lr=0.001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=10, gamma=0.1)\n",
    "writer.add_graph(model_conv, inputs.to(device))\n",
    "model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=20)\n",
    "visualize_model(model_conv,1)\n",
    "time.sleep(2)\n",
    "random_imgs = random.sample(os.listdir(\"./Dataset/Test/\"), 1)\n",
    "\n",
    "for img in random_imgs:\n",
    "    visualize_model_predictions(model_conv, img_path=f'./Dataset/Test/{img}')\n",
    "    time.sleep(2)\n",
    "    plt.close(\"all\")\n",
    "\n",
    "correct = {x:0 for x in class_names}\n",
    "wrong  = {x:0 for x in class_names}\n",
    "total = {x:0 for x in class_names}\n",
    "\n",
    "for img_path in glob.glob(\"./Dataset/Test/*\"):\n",
    "    was_training = model_conv.training\n",
    "    model_conv.eval()\n",
    "\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    label = img_path.split('/')[-1].split(\"_\")[0]\n",
    "    label = class_to_idx[label]\n",
    "    img = data_transforms[\"Validation\"](image=img)[\"image\"]\n",
    "    img = img.unsqueeze(0)\n",
    "    img = img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model_conv(img)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        model_conv.train(mode=was_training)\n",
    "\n",
    "    if preds[0] == label:\n",
    "        correct[class_names[label]] += 1\n",
    "    elif preds[0] != label:\n",
    "        wrong[class_names[label]] +=1\n",
    "    total[class_names[label]] += 1\n",
    "\n",
    "print(f\"Test acc : {sum(correct.values())/sum(total.values())*100:.2f}%\\n\")\n",
    "print(f\"Correct : {correct}\")\n",
    "print(f\"Wrong : {wrong}\")\n",
    "print(f\"Total : {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "##### ============================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CUDA!\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pandas.core.common import flatten\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Running on CUDA!\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"Running on CPU!\")\n",
    "    \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "                        nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels),\n",
    "                        nn.ReLU())\n",
    "        self.conv2 = nn.Sequential(\n",
    "                        nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1),\n",
    "                        nn.BatchNorm2d(out_channels))\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes = 10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size = 7, stride = 2, padding = 3), nn.BatchNorm2d(64), nn.ReLU())\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.layer0 = self.make_layer(block, 64, layers[0], stride = 1)\n",
    "        self.layer1 = self.make_layer(block, 128, layers[1], stride = 2)\n",
    "        self.layer2 = self.make_layer(block, 256, layers[2], stride = 2)\n",
    "        self.layer3 = self.make_layer(block, 512, layers[3], stride = 2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(planes),\n",
    "            )\n",
    "            \n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer0(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(384),\n",
    "            nn.ReLU())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size = 3, stride = 2))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(9216, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU())\n",
    "        self.fc2= nn.Sequential(\n",
    "            nn.Linear(4096, num_classes))\n",
    "      \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "class_names = []\n",
    "test_image_paths = []\n",
    "train_image_paths = []\n",
    "\n",
    "train_data_path = \"./Dataset/Train/\" \n",
    "test_data_path = \"./Dataset/Validation/\"\n",
    "\n",
    "\n",
    "for data_path in glob.glob(train_data_path + '/*'):\n",
    "    class_names.append(data_path.split('/')[-1]) \n",
    "    train_image_paths.append(glob.glob(data_path + '/*'))\n",
    "    \n",
    "for data_path in glob.glob(test_data_path + '/*'):\n",
    "    test_image_paths.append(glob.glob(data_path + '/*'))\n",
    "\n",
    "\n",
    "train_image_paths = list(flatten(train_image_paths))\n",
    "random.shuffle(train_image_paths)\n",
    "train_image_paths = train_image_paths[:int(0.8*len(train_image_paths))]\n",
    "valid_image_paths = train_image_paths[int(0.8*len(train_image_paths)):]\n",
    "\n",
    "test_image_paths = list(flatten(test_image_paths))\n",
    "\n",
    "idx_to_class = {i:j for i, j in enumerate(class_names)}\n",
    "class_to_idx = {value:key for key,value in idx_to_class.items()}\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filepath = self.image_paths[idx]\n",
    "        image = cv2.imread(image_filepath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        label = image_filepath.split('/')[-2]\n",
    "        label = class_to_idx[label]\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)[\"image\"]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "\n",
    "EPOCHS = 10\n",
    "BATCH = 32\n",
    "LR = 0.001\n",
    "IMG_SIZE = 224\n",
    "PRIME_TRANSFORMS = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "RANDOM_TRANSFORMS =  A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=0.5),\n",
    "    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.MultiplicativeNoise(multiplier=[0.5,2], per_channel=True, p=0.2),\n",
    "    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "train_dataset = Dataset(train_image_paths, PRIME_TRANSFORMS)\n",
    "valid_dataset = Dataset(valid_image_paths, PRIME_TRANSFORMS)\n",
    "test_dataset = Dataset(test_image_paths, PRIME_TRANSFORMS)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\\nValid size: {len(valid_dataset)}\\nTest size: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH, shuffle=True) \n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "resnet_model = ResNet(ResidualBlock, [3, 4, 6, 3]).to(device)\n",
    "resnet_criterion = nn.CrossEntropyLoss()\n",
    "resnet_optimizer = torch.optim.SGD(resnet_model.parameters(), lr=LR, weight_decay=0.001, momentum =0.9)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = resnet_model(images)\n",
    "        loss = resnet_criterion(outputs, labels)\n",
    "        \n",
    "        resnet_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        resnet_optimizer.step()\n",
    "        del images, labels, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print (f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in valid_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = resnet_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print(f'Accuracy of the network on the {len(valid_dataset)} validation images: {100 * correct / total:.2f} %')\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = resnet_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print(f'Accuracy of the network on the {len(test_dataset)} test images: {100 * correct / total:.2f} %')\n",
    "\n",
    "\n",
    "CLASSES = 10\n",
    "EPOCHS = 40\n",
    "BATCH = 128\n",
    "LR = 0.01\n",
    "IMG_SIZE = 227\n",
    "RANDOM_TRANSFORMS =  A.Compose([\n",
    "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=360, p=0.5),\n",
    "    A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.MultiplicativeNoise(multiplier=[0.5,2], per_channel=True, p=0.3),\n",
    "    A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE, p=1),\n",
    "    A.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010], p=1),\n",
    "    ToTensorV2(p=1),\n",
    "])\n",
    "PRIME_TRANSFORMS = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE, p=1),\n",
    "    A.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010], p=1),\n",
    "    ToTensorV2(p=1),\n",
    "])\n",
    "\n",
    "train_dataset = Dataset(train_image_paths, RANDOM_TRANSFORMS)\n",
    "valid_dataset = Dataset(valid_image_paths, PRIME_TRANSFORMS)\n",
    "test_dataset = Dataset(test_image_paths, PRIME_TRANSFORMS)\n",
    "\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}\\nValid size: {len(valid_dataset)}\\nTest size: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH, shuffle=False)\n",
    "\n",
    "alexnet_model = AlexNet(CLASSES).to(device)\n",
    "alexnet_criterion = nn.CrossEntropyLoss()\n",
    "alexnet_optimizer = torch.optim.SGD(alexnet_model.parameters(), lr=LR, weight_decay = 0.005, momentum = 0.9)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = alexnet_model(images)\n",
    "        loss = alexnet_criterion(outputs, labels)\n",
    "\n",
    "        alexnet_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        alexnet_optimizer.step()\n",
    "\n",
    "    print (f'Epoch [{epoch+1}/{EPOCHS}], Loss: { loss.item():.4f}')\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     correct = 0\n",
    "    #     total = 0\n",
    "    #     for images, labels in valid_loader:\n",
    "    #         images = images.to(device)\n",
    "    #         labels = labels.to(device)\n",
    "    #         outputs = alexnet_model(images)\n",
    "    #         _, predicted = torch.max(outputs.data, 1)\n",
    "    #         total += labels.size(0)\n",
    "    #         correct += (predicted == labels).sum().item()\n",
    "    #         del images, labels, outputs\n",
    "    \n",
    "    #     print(f'Accuracy of the network on the {len(valid_dataset)} validation images: {100 * correct / total:.2f} %')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = alexnet_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        del images, labels, outputs\n",
    "\n",
    "    print(f'Accuracy of the network on the {len(test_dataset)} test images: {100 * correct / total:.2f} %')   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
